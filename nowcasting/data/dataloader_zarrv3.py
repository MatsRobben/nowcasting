import zarr
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import lightning as L

from omegaconf import OmegaConf

from typing import Optional
import collections
import warnings

# Import custom transformation and sample generation utilities
from nowcasting.data.utils.transforms import set_transforms
from nowcasting.data.utils.generate_samples import get_sample_dfs

def scale_slice(slc: slice, factor: int) -> slice:
    """Scale slice indices by dividing by factor."""
    return slice(
        slc.start // factor if slc.start is not None else None,
        slc.stop // factor if slc.stop is not None else None,
        slc.step // factor if slc.step is not None else None
    )

class NowcastingDataset(Dataset):
    """
    A PyTorch Dataset for loading and processing nowcasting data from a ZarrV3 store.

    This dataset handles the retrieval of context (input) and forecast (target)
    sequences, applying various transformations, and supporting flexible input
    structures (e.g., for multi-modal models). It operates on a DataFrame of
    sample indices generated by `get_sample_dfs`.

    Parameters:
        df : pd.DataFrame
            A DataFrame where each row represents a data sample. It must contain
            't_idx', 'h_idx', and 'w_idx' columns, which are the base time,
            height block, and width block indices for the sample.
        root : zarr.Group
            The opened Zarr group object representing the root of the dataset.
            This is used to access the actual data arrays and their attributes.
        info : dict
            A dictionary containing configuration details for data loading and
            transformation. Expected keys include:
            - 'in_vars' (list): Specifies input variables. Can be a flat list
              (e.g., `["radar/rtcor"]`) or a nested list (e.g., `[["radar/rtcor"], ["sat/IR"]]`)
              to group variables for multi-input models.
            - 'out_vars' (list): Specifies output (target) variables.
            - 'transforms' (dict): A dictionary mapping Zarr group names or
              full variable paths to callable transformation functions (e.g.,
              `{"radar": <transform_func>}`). These functions are typically
              set up by `set_transforms`.
            - 'latlon' (bool, optional): If True, includes latitude and longitude
              maps as additional channels/groups in the context data.
        size : tuple[int, int], default (8, 8)
            The spatial dimensions of the extracted data patch, in units of `block_size`.
            For example, `(8, 8)` means 8 blocks in height and 8 blocks in width.
        block_size : int, default 32
            The size (in pixels/grid cells) of a single spatial block. The total
            spatial dimension of a sample will be `size[0] * block_size` by
            `size[1] * block_size`.
        context_len : int, default 4
            The number of time steps to include in the context (input) sequence.
        forecast_len : int, default 18
            The number of time steps to include in the forecast (target) sequence.
        include_timestamps : bool, default False
            If True, a 1D NumPy array of relative timestamps (from `-(context_len-1)` to `0`)
            is included with the context data. For nested `in_vars`, it's added
            to each group; otherwise, it's a separate tensor.
    """
    def __init__(
        self,
        df: pd.DataFrame,
        root: zarr.Group,
        info: dict,
        size: tuple[int, int] | list[tuple[int, int]] = (8, 8),
        block_size: int = 32,
        context_len: int = 4,
        forecast_len: int = 18,
        include_timestamps: bool = False,
        use_crop: bool | collections.abc.Sequence = True
    ):
        self.df = df
        self.root = root
        self.info = info
        self.size = size
        self.block_size = block_size
        self.context_len = context_len
        self.forecast_len = forecast_len
        self.include_timestamps = include_timestamps
        self.use_crop = use_crop

        # Analyze input/output structure
        self._setup_variables()
        self._validate_crop_parameter()
        self._validate_size() 
        
        # Load static lat/lon data if needed
        self._load_static_data()

    def _validate_size(self):
        """Validate the size parameter and prepare it for nested use."""
        in_vars = self.info.get('in_vars', [])
        num_input_groups = len(in_vars)

        # Check if the size is already in a nested format
        is_size_nested = isinstance(self.size, collections.abc.Sequence) and any(isinstance(x, collections.abc.Sequence) for x in self.size)
        
        if is_size_nested:
            # Validate length for pre-nested size
            if len(self.size) != num_input_groups:
                raise ValueError(
                    f"Length of `size` list ({len(self.size)}) must match the number of "
                    f"input groups ({num_input_groups})."
                )
        else:
            # If size is not nested, make it nested.
            # Handle cases with no input groups as a single group.
            if num_input_groups == 0:
                self.size = [self.size]
            else:
                self.size = [self.size] * num_input_groups

    def _validate_crop_parameter(self):
        """Validate use_crop parameter against nested group structure."""
        if not self.nested_input and self.use_crop:
            warnings.warn("use_crop is only supported for nested input structures. Ignoring.")
            self.use_crop = [True]
            return
            
        if isinstance(self.use_crop, collections.abc.Sequence):
            if len(self.use_crop) != len(self.info['in_vars']):
                raise ValueError(
                    "Length of use_crop list must match number of nested input groups. "
                    f"Expected {len(self.info['in_vars'])}, got {len(self.use_crop)}"
                )
        elif self.use_crop and self.nested_input:
            # If single boolean True, apply to all groups
            self.use_crop = [True] * len(self.info['in_vars'])

    def _setup_variables(self):
        """Analyze variable structure and identify optimization opportunities."""
        in_vars = self.info.get('in_vars', [])
        out_vars = self.info.get('out_vars', [])
        
        # Determine if inputs are nested (multi-modal)
        self.nested_input = len(in_vars) > 0 and isinstance(in_vars[0], collections.abc.Sequence)
        
        # Get main variable for metadata
        self.main_var = in_vars[0][0] if self.nested_input else (in_vars[0] if in_vars else None)
        self.main_var_shape = self.root[self.main_var].shape if self.main_var else None
        
        # Create a map of each input variable to its group index
        self.in_var_to_group = {}
        if self.nested_input:
            for i, group in enumerate(in_vars):
                for var in group:
                    self.in_var_to_group[var] = i
        else:
            # For non-nested inputs, each variable is in group 0
            for var in in_vars:
                self.in_var_to_group[var] = 0


        # Find variables that appear in both input and output (optimization target)
        flat_in_vars = []
        if self.nested_input:
            for group in in_vars:
                flat_in_vars.extend(group)
        else:
            flat_in_vars = in_vars.copy()
        
        self.overlapping_vars = set(flat_in_vars) & set(out_vars)

    def _load_static_data(self):
        """Load static latitude/longitude data if requested."""
        if not self.info.get('latlon', False):
            return
            
        main_group = self.main_var.split('/')[0] if self.main_var else None
        if main_group and main_group in self.root:
            self.lat = self.root[main_group]['lat'][:].astype(np.float32)
            self.lon = self.root[main_group]['lon'][:].astype(np.float32)
        else:
            warnings.warn("latlon requested but no main variable available")

    def __len__(self):
        return len(self.df)

    def _create_spatial_slices(self, h_idx: int, w_idx: int) -> tuple[slice, slice] | list[tuple[slice, slice]]:
        """Create spatial slice(s) based on the size configuration."""
        def get_slice_for_size(size_tuple):
            h0 = h_idx * self.block_size
            w0 = w_idx * self.block_size
            h1 = h0 + size_tuple[0] * self.block_size
            w1 = w0 + size_tuple[1] * self.block_size
            return (slice(h0, h1), slice(w0, w1))

        return [get_slice_for_size(s) for s in self.size]
    
    def _create_spatial_slices(self, h_idx: int, w_idx: int) -> list[tuple[slice, slice]]:
        """Create spatial slice(s) based on the size and crop configuration."""
        
        spatial_slices = []
        for size, crop_flag in zip(self.size, self.use_crop):
            if crop_flag:
                # Use the randomized indices from the DataFrame
                h0 = h_idx * self.block_size
                w0 = w_idx * self.block_size
                h1 = h0 + size[0] * self.block_size
                w1 = w0 + size[1] * self.block_size
            else:
                # Use a fixed index of 0 to get a non-randomized, full-image crop
                h0 = 0
                w0 = 0
                # Use the full size of the group
                h1 = size[0] * self.block_size
                w1 = size[1] * self.block_size
            
            spatial_slices.append((slice(h0, h1), slice(w0, w1)))

        return spatial_slices

    def _get_temporal_slice(self, var: str, time_start: int, time_end: int) -> tuple:
        """Get appropriate temporal slice for a variable based on its group properties."""
        group = var.split('/')[0]
        t_scale = self.root[group].attrs['interval_minutes'] // 5
        
        # Handle special case for Harmonie forecast data
        if group == 'harmonie':
            forecast_start = self.root[group].attrs['forecast_range'][0]
            t0_adj = time_start - forecast_start * 12
            t1_adj = time_end - forecast_start * 12
            forecast_idx = max(t0_adj // t_scale, 0)
            time_range = (np.arange(t0_adj, t1_adj) - (forecast_idx * t_scale)) // 12
            return (forecast_idx, slice(time_range[0], time_range[-1] + 1)), time_range
        else:
            time_range = np.arange(time_start, time_end) // t_scale
            return (slice(time_range[0], time_range[-1] + 1),), time_range

    def _get_spatial_slice(self, var: str, spatial_slice: tuple[slice, slice]) -> tuple:
        """Get appropriate spatial slice for a variable based on its group properties."""
        group = var.split('/')[0]
        
        if group == 'aws':
            return (slice(None),)  # AWS data loads all stations
        else:
            img_scale = self.root[group].attrs['downscale_factor']
            return tuple(scale_slice(s, img_scale) for s in spatial_slice)
        
    def _load_variable_data(self, var: str, time_start: int, time_end: int, 
                          spatial_slice: tuple[slice, slice]) -> np.ndarray:
        """Load data for a single variable with all preprocessing applied."""
        group = var.split('/')[0]
        
        # Get temporal and spatial slices
        time_slice, time_range = self._get_temporal_slice(var, time_start, time_end)
        scaled_spatial_slice = self._get_spatial_slice(var, spatial_slice)
        
        # Handle channel dimension for Harmonie data (Temp fix)
        has_channel = group == "harmonie" and len(self.root[var].shape) == 5
        if has_channel:
            data_slice = time_slice + (slice(0, 1),) + scaled_spatial_slice
        else:
            data_slice = time_slice + scaled_spatial_slice

        # Load raw data
        data = self.root[var][data_slice].astype(np.float32)
        
        if has_channel:
            data = np.squeeze(data, axis=1)

        # Handle AWS data (load locations too)
        if group == 'aws':
            locs = self.root[var + '_loc'][:]
            data = (data, locs)

        # Apply transformations
        for transform_key in [group, var]:
            if transform_key in self.info.get('transforms', {}):
                data = self.info['transforms'][transform_key](data)

        # Post-transformation AWS handling
        if group == 'aws' and hasattr(data, 'shape') and data.shape[-2:] == self.main_var_shape[-2:]:
            crop_slice = (slice(None),) + spatial_slice
            data = data[crop_slice]

        # Handle temporal resampling
        _, inverse_indices = np.unique(time_range, return_inverse=True)
        return data[inverse_indices]
    
    def _ensure_channel_dim(self, data: np.ndarray) -> np.ndarray:
        """Ensure data has channel dimension: (T,H,W) -> (1,T,H,W)."""
        return np.expand_dims(data, axis=0) if data.ndim == 3 else data
    
    def _split_temporal_data(self, data: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        """Split full temporal data into context and forecast portions."""
        context_data = data[:, :self.context_len, :, :]
        forecast_data = data[:, self.context_len:self.context_len+self.forecast_len, :, :]
        return context_data, forecast_data
    
    def _create_latlon_data(self, spatial_slice: tuple[slice, slice], time_len: int) -> np.ndarray:
        """Create lat/lon data for the spatial slice and time length."""
        if not hasattr(self, 'lat') or self.lat is None:
            warnings.warn("Latitude/Longitude data not available")
            return np.empty((2, time_len, 0, 0), dtype=np.float32)

        lat_crop = self.lat[spatial_slice]
        lon_crop = self.lon[spatial_slice]
        
        lat_broadcast = np.broadcast_to(lat_crop, (time_len, *lat_crop.shape))
        lon_broadcast = np.broadcast_to(lon_crop, (time_len, *lon_crop.shape))

        return np.stack([lat_broadcast, lon_broadcast], axis=0)
    
    def _load_context_data(self, t0: int, t1: int, spatial_slices: list[tuple],
                           full_data_cache: dict, crop_data) -> any:
        """Load and format context (input) data."""
        in_vars = self.info.get('in_vars', [])
        if not in_vars:
            return None

        # Generate timestamps if needed
        context_timesteps = None
        if self.include_timestamps:
            context_timesteps = np.arange(-self.context_len + 1, 1, dtype=np.float32)

        if self.nested_input:
            return self._load_nested_context(in_vars, t0, t1, spatial_slices,
                                            full_data_cache, context_timesteps, crop_data)
        else:
            return self._load_flat_context(in_vars, t0, t1, spatial_slices[0], 
                                         full_data_cache)
        
    def _load_nested_context(self, in_vars: list, t0: int, t1: int, spatial_slices: tuple,
                           full_data_cache: dict, context_timesteps: np.ndarray,
                           crop_data: np.ndarray) -> list:
        """Load context data in nested format (multi-modal)."""
        context = []
        
        # Process each variable group
        for i, group in enumerate(in_vars):
            spatial_slice = spatial_slices[i]

            group_data_list = []
            for var in group:
                if var in full_data_cache:
                    # Use cached full-range data
                    full_data = self._ensure_channel_dim(full_data_cache[var])
                    context_part, _ = self._split_temporal_data(full_data)
                    group_data_list.append(context_part)
                else:
                    # Load context portion only
                    data = self._load_variable_data(var, t0, t1, spatial_slice)
                    data = self._ensure_channel_dim(data)
                    group_data_list.append(data)
            
            # Concatenate group variables and create element
            group_data = np.concatenate(group_data_list, axis=0)
            element = [group_data]

            # Add timestamps if requested
            if self.include_timestamps:
                element.append(context_timesteps.copy())
            
            # Add crop info if requested for this group
            if isinstance(self.use_crop, collections.abc.Sequence) and not self.use_crop[i]:
                element.append(crop_data.copy())

            context.append(element)
        
        # Add lat/lon as separate group if requested
        if self.info.get('latlon', False) and hasattr(self, 'lat'):
            latlon_data = self._create_latlon_data(spatial_slice, self.context_len)
            element = [latlon_data]
            if self.include_timestamps:
                element.append(context_timesteps.copy())
            context.append(element)
        
        return context
    
    def _load_flat_context(self, in_vars: list, t0: int, t1: int, spatial_slice: tuple,
                         full_data_cache: dict) -> np.ndarray:
        """Load context data in flat format (single concatenated tensor)."""
        context_data_list = []
        
        for var in in_vars:
            if var in full_data_cache:
                # Use cached full-range data
                full_data = self._ensure_channel_dim(full_data_cache[var])
                context_part, _ = self._split_temporal_data(full_data)
                context_data_list.append(context_part)
            else:
                # Load context portion only
                data = self._load_variable_data(var, t0, t1, spatial_slice)
                data = self._ensure_channel_dim(data)
                context_data_list.append(data)
        
        context = np.concatenate(context_data_list, axis=0)
        
        # Add lat/lon if requested
        if self.info.get('latlon', False) and hasattr(self, 'lat'):
            latlon_data = self._create_latlon_data(spatial_slice, self.context_len)
            context = np.concatenate([context, latlon_data], axis=0)
        
        return context
    
    def _load_future_data(self, t1: int, t2: int, spatial_slice: tuple, 
                         full_data_cache: dict) -> np.ndarray:
        """Load and format future (target) data."""
        out_vars = self.info.get('out_vars', [])
        if not out_vars:
            return None

        future_data_list = []
        for var in out_vars:
            if var in full_data_cache:
                # Use cached full-range data
                full_data = self._ensure_channel_dim(full_data_cache[var])
                _, forecast_part = self._split_temporal_data(full_data)
                future_data_list.append(forecast_part)
            else:
                # Load forecast portion only
                data = self._load_variable_data(var, t1, t2, spatial_slice)
                data = self._ensure_channel_dim(data)
                future_data_list.append(data)
        
        return np.concatenate(future_data_list, axis=0)
    
    def __getitem__(self, idx: int):
        """
        Retrieve a single optimized data sample.
        
        Main optimization: Load overlapping variables once with full temporal range,
        then split into context/forecast portions as needed.
        """
        # Extract sample coordinates
        t_idx, h_idx, w_idx = self.df.loc[idx, ['t_idx', 'h_idx', 'w_idx']].astype(int)

        # Calculate time ranges and spatial slice
        t0 = t_idx
        t1 = t0 + self.context_len
        t2 = t1 + self.forecast_len
        
        spatial_slices = self._create_spatial_slices(h_idx, w_idx)

        # Prepare crop data
        crop_data = np.array([h_idx, w_idx, self.size[0][0], self.size[0][1]], dtype=np.float32)

        # Load overlapping variables once with full temporal range
        full_data_cache = {}
        for var in self.overlapping_vars:
            group_idx = self.in_var_to_group[var]
            full_data_cache[var] = self._load_variable_data(var, t0, t2, spatial_slices[group_idx])

        # Load context and future data using optimized strategy
        context = self._load_context_data(t0, t1, spatial_slices, full_data_cache, crop_data)
        future = self._load_future_data(t1, t2, spatial_slices[0], full_data_cache)

        # Return appropriate format
        if context is not None and future is not None:
            return context, future
        elif context is not None:
            return context
        elif future is not None:
            return future
        else:
            raise ValueError("Both context and future data are empty")

class NowcastingDataModule(L.LightningDataModule):
    """
    A PyTorch Lightning DataModule for nowcasting datasets.

    This DataModule encapsulates all data-related logic, including:
    - Loading data from a Zarr store.
    - Defining dataset splits (train, validation, test) based on time rules.
    - Applying global time masks (e.g., for missing data or clutter).
    - Setting up data transformations.
    - Generating data samples (patches) and optionally applying weighted sampling
      for the training set.
    - Providing PyTorch DataLoaders for each split.

    It integrates with `NowcastingDataset` for sample loading and `get_sample_dfs`
    and `set_transforms` from the `nowcasting.data.utils` module.

    Parameters:
        path : str
            The file path to the root Zarr store containing the meteorological data.
        var_info : dict
            A dictionary defining variables to load, their transformations, and
            which variable to use for sample weighting. See `NowcastingDataset`
            `info` parameter for more details.
        split_info : dict
            A dictionary defining how to split the dataset temporally, and
            which masks to apply. Expected keys:
            - 'split_rules' (dict): Rules for assigning timestamps to 'train',
              'val', 'test' splits. Each key corresponds to a split name (e.g.,
              "train", "val", "test"), and its value is a dictionary specifying
              the temporal criteria for that split.
              
              Example: `{"test": {"year": [2023]}, "val": {"month": [6, 11]}, "train": {}}`
              
              * **Temporal Attributes:** Keys within a split's rule (e.g., "year", "month", "day",
                  "hour", "day_of_week") correspond to attributes of a `pandas.Timestamp`.
              * **List of Values:** The values for these attributes are lists of integers. A timestamp
                  matches the rule if its corresponding temporal attribute is present in the list.
              * **AND Logic:** If multiple temporal attributes are specified within one rule (e.g.,
                  `{"year": [2023], "month": [1]}`), a timestamp must satisfy *all* conditions.
              * **OR Logic:** If a list contains multiple values (e.g., `{"month": [6, 11]}`), a
                  timestamp matches if its month is 6 *or* 11.
              * **Prioritization:** Rules are typically applied in a defined order (e.g., 'test' and
                  'val' rules might be applied first, with a default empty `{}` rule for 'train'
                  catching all remaining timestamps). This ensures clean separation and avoids
                  data leakage.

            - 'apply_missing_masks' (list[str], optional): List of Zarr group
              names whose 'time_mask' attributes should be used to filter invalid
              timestamps. Where True means that a time index in pressent and False that it is missing.
            - 'clutter_threshold' (float, optional): A threshold for radar 'clutter_score'. 
              A pixel is marked as clutter if its gradient exceeds 30 mm/h.  If the number of 
              clutter pixels in a sample exceeds the 'clutter_threshold', the sample is masked.
              
        sample_info : dict, optional
            Configuration for sample generation and weighted sampling. Expected keys:
            - 'threshold' (float, optional): Filters samples based on their 'weight'
              from `sample_var`.
            - 'methods' (dict): Configuration for `get_sample_dfs` to determine
              how sample weights are aggregated (e.g., `{'train': {'agg': 'max_pool'}}`).
            - 'bins' (dict, optional): Configuration for `WeightedRandomSampler`
              for the training set, including `n_bins`, `first_bin`, `last_bin`,
              `slope`, `scaler`, `n_samples`.

        context_len : int, default 4
            Number of time steps for the input sequence.
        forecast_len : int, default 18
            Number of time steps for the output sequence.
        include_timestamps : bool, default False
            If True, relative timestamps are included in the context data.
        img_size : tuple[int, int], default (8, 8)
            The spatial dimensions of the extracted image patch in `block_size` units.
        stride : tuple[int, int, int], default (1, 1, 1)
            The stride (temporal, height, width) for generating samples.
        batch_size : int, default 16
            The batch size for DataLoaders.
        num_workers : int, default 8
            Number of subprocesses to use for data loading. Set to 0 for single-process.
    """
    def __init__(
        self,
        path: str,
        var_info: dict[str, object],
        split_info: dict[str, object],
        sample_info: dict[str, object] = {},
        context_len: int = 4,
        forecast_len: int = 18,
        include_timestamps: bool = False,
        use_crop: bool | collections.abc.Sequence = False,
        img_size: collections.abc.Sequence[int, int] | collections.abc.Sequence[tuple[int, int]] = (8, 8),
        stride: collections.abc.Sequence[int, int, int] = (1, 1, 1),
        batch_size: int = 16,
        num_workers: int = 8,
    ):

        super().__init__()
        self.path = path
        self.var_info = var_info
        self.split_info = split_info
        self.context_len = context_len
        self.forecast_len = forecast_len
        self.include_timestamps = include_timestamps
        self.use_crop = use_crop
        self.img_size = img_size
        self.stride = stride
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.sample_info = sample_info
        self.sampler = None # Sampler for training data

        # Resolve standard python dicts to OmegaConf objects if they are passed as such.
        # This ensures consistent dictionary access.
        if not isinstance(self.var_info, dict):
            self.var_info = OmegaConf.to_container(self.var_info, resolve=True)
        if not isinstance(self.split_info, dict):
            self.split_info = OmegaConf.to_container(self.split_info, resolve=True)
        if not isinstance(self.sample_info, dict):
            self.sample_info = OmegaConf.to_container(self.sample_info, resolve=True)

    def setup(self, stage: Optional[str] = None):
        """
        Prepares the datasets for training, validation, and testing.

        This method is called by PyTorch Lightning at the beginning of training,
        validation, or testing. It performs the following steps:
        1. Opens the Zarr data store.
        2. Initializes data transformations.
        3. Gathers temporal metadata.
        4. Creates a global time mask based on data availability and clutter.
        5. Generates data sample DataFrames for each split (train, val, test).
        6. Instantiates `NowcastingDataset` for each split.
        7. Optionally creates a `WeightedRandomSampler` for the training set.

        Parameters:
            stage : str, optional
                The current stage ('fit', 'validate', 'test', 'predict').
                This parameter is typically ignored as `setup` prepares all splits.
        """
        # Open the Zarr root group in read-only mode.
        root = zarr.open_group(self.path, mode='r')

        # Determine the main variable and its group to extract metadata.
        in_vars = self.var_info.get('in_vars', [])
        nested_output = len(in_vars) > 0 and isinstance(in_vars[0], (list, tuple))
        main_var = in_vars[0][0] if nested_output else (in_vars[0] if in_vars else None)
        main_group = main_var.split('/')[0]

        if main_var is None:
            raise ValueError("No 'in_vars' defined in var_info. Cannot determine main variable for setup.")

        # Convert transform configurations (names and params) into callable functions.
        # This modifies `self.var_info["transforms"]` in-place.
        set_transforms(self.var_info["transforms"])

        # Get temporal metadata from the main Zarr group
        start_time = root[main_group].attrs['start_time']
        end_time = root[main_group].attrs['end_time']
        min_interval = 5 #root[main_group].attrs['interval_minutes']
        timestamps = pd.date_range(start_time, end_time, freq=f"{min_interval}min")

        # Define the full kernel size for sample generation: (temporal_length, height, width).
        if isinstance(self.img_size[0], collections.abc.Sequence):
            # img_size is nested, use the first element
            kernel = (self.context_len + self.forecast_len,) + tuple(self.img_size[0])
        else:
            # img_size is a simple tuple/list, use it directly
            kernel = (self.context_len + self.forecast_len,) + self.img_size

        # Load the array used for sample weighting (e.g., max intensity grid).
        sample_array_path = self.var_info.get('sample_var')
        if sample_array_path is None:
            raise ValueError("Missing 'sample_var' in var_info. Cannot generate samples.")
        
        try:
            sample_array = root[sample_array_path][:]
        except KeyError:
            raise KeyError(f"Sample variable '{sample_array_path}' not found in Zarr store.")

        # Create a global time mask to filter out missing data or high clutter periods.
        time_mask = self._create_time_mask(root, len(sample_array))

        # Generate DataFrames for each split (train, val, test) containing sample indices and weights.
        dfs = get_sample_dfs(
            sample_array=sample_array, 
            kernel=kernel,
            stride=self.stride, 
            timestamps=timestamps,
            split_rules=self.split_info['split_rules'],
            methods=self.sample_info.get('methods', {}),
            time_mask=time_mask
        )
        
        # Instantiate NowcastingDataset for each split.
        self.datasets = {}
        for name, df in dfs.items():
            # Apply an optional threshold filter to the sample weights.
            if 'threshold' in self.sample_info:
                initial_len = len(df)
                # The threshold is applied to `df['weight']`. The threshold value is multiplied by (255/50)
                # to reverse the applied scaling, from 0-255 to 0-50 mm-h. 
                df = df[df['weight'] > (self.sample_info['threshold'] * (255/50))] # Adjust scaler if your weight unit is different
                df.reset_index(drop=True, inplace=True) # Reset index after filtering
                print(f"Split '{name}': {initial_len} samples initially, {len(df)} after weight thresholding.")
            
            # Print some info about the created DataFrame (e.g., length, max weight sample index)
            if not df.empty:
                print(f"Split '{name}': Total samples: {len(df)}, Max weight sample index: {df['weight'].idxmax()}")
            else:
                print(f"Split '{name}': No samples found after filtering.")

            self.datasets[name] = NowcastingDataset(
                df=df,
                root=root,
                info=self.var_info,
                size=self.img_size,
                context_len=self.context_len,
                forecast_len=self.forecast_len,
                include_timestamps=self.include_timestamps,
                use_crop=self.use_crop
            )
            # Create a weighted sampler only for the training dataset if 'bins' are configured.
            if name == 'train' and 'bins' in self.sample_info:
                self._create_sampler(df, self.sample_info['bins'])

    def _create_time_mask(self, root: zarr.Group, num_timesteps: int) -> np.ndarray:
        """
        Generates a global boolean time mask based on missing data and clutter scores.

        This internal helper function combines multiple masking criteria to identify
        valid time steps across the entire dataset.

        Parameters:
            root : zarr.Group
                The root Zarr group of the dataset.
            num_timesteps : int
                The total number of time steps in the global time axis.

        Returns:
            np.ndarray
                A 1D boolean NumPy array of length `num_timesteps`. `True` indicates
                a valid time step, `False` indicates a masked (invalid) time step.
        """
        global_mask = np.ones(num_timesteps, dtype=bool)
        
        # Apply masks for missing data from specified Zarr groups.
        # It iterates through groups listed in `apply_missing_masks` and uses their
        # `time_mask` datasets, adjusting for different temporal intervals.
        for group_name in self.split_info.get('apply_missing_masks', []):
            if group_name not in root:
                warnings.warn(f"Missing data mask requested for group '{group_name}', but it's not found in Zarr root. Skipping.")
                continue

            group = root[group_name]
            # Get the temporal scaling factor for this group relative to the 5-minute global interval.
            factor = group.attrs['interval_minutes'] // 5
            
            if 'time_mask' not in group:
                warnings.warn(f"Group '{group_name}' does not have a 'time_mask' dataset. Skipping its missing data mask.")
                continue

            group_mask = group['time_mask'][:]
            
            # Repeat the group's mask to match the global 5-minute granularity
            # and apply it as a logical AND to the global mask.
            global_mask &= np.repeat(group_mask, factor)

        # Apply mask based on radar clutter score.
        # Samples with a clutter score above `clutter_threshold` are masked out.
        if 'radar' in root and 'clutter_score' in root['radar']:
            clutter = root['radar']['clutter_score'][:]
            # Use `float('inf')` as default for `clutter_threshold` if not specified,
            # effectively disabling this mask unless explicitly set.
            clutter_threshold = self.split_info.get('clutter_threshold', float('inf'))
            clutter_mask = clutter < clutter_threshold
            global_mask &= clutter_mask
        else:
            warnings.warn("Radar group or 'clutter_score' dataset not found. Skipping clutter masking.")

        return global_mask

    def _create_sampler(self, df: pd.DataFrame, cfg: dict[str, object]):
        """
        Creates a `WeightedRandomSampler` for the training data based on sample weights.

        This method is used to implement a form of importance sampling, typically
        to oversample rare but important events (e.g., heavy rainfall). It bins
        the sample 'weight' values and assigns inverse frequency weights.

        Parameters:
            df : pd.DataFrame
                The DataFrame of training samples, containing the 'weight' column.
            cfg : dict
                Configuration dictionary for the sampler, expected to contain:
                - 'first_bin' (float): The lower bound for the first bin.
                - 'last_bin' (float): The upper bound for the last bin.
                - 'n_bins' (int): Number of bins to discretize sample weights.
                - 'slope' (float): Controls the spacing of bins (e.g., for logarithmic spacing).
                - 'scaler' (float): A scaling factor applied to `df['weight']` before binning.
                - 'n_samples' (int, optional): The total number of samples to draw per epoch.
                  Defaults to the length of the DataFrame.
        """
        first_bin = cfg.get('first_bin', 0.2)
        last_bin = cfg.get('last_bin', 12.0)
        n_bins = cfg.get('n_bins', 8)
        slope = cfg.get('slope', 1.0)
        scaler = cfg.get('scaler', 255/50) # Default scaler, specific to radar intensity 0-255 -> 0-50 mm-h
        n_samples = cfg.get('n_samples', len(df))

        # Compute bin edges. The example uses an exponential (logarithmic) spacing
        # for bins, which is common for skewed distributions like rainfall intensity.
        # The `scaler` is applied to `first_bin` and `last_bin` to match the units
        # of the `df['weight']` column.
        edges = np.exp(np.linspace(np.log(first_bin * scaler), 
                                   np.log(last_bin * scaler), 
                                   n_bins - 1))
        
        # Re-calculating edges based on a base_edges and slope, which seems to be a more
        # flexible way to define non-linear bin spacing.
        base_edges = np.linspace(0, 1, n_bins - 1)
        edges = np.exp(np.log(first_bin) + (np.log(last_bin) - np.log(first_bin)) * (base_edges ** slope))

        # Digitize (bin) the 'weight' values from the DataFrame.
        bin_idx = np.digitize(df['weight'].values, edges)

        # Count samples in each bin. `minlength=n_bins` ensures all bins are represented, even if empty.
        counts = np.bincount(bin_idx, minlength=n_bins)

        # Avoid division by zero for empty bins by setting their count to 1.
        counts = np.where(counts == 0, 1, counts)

        # Calculate inverse frequency weights: rarer bins get higher weights.
        weights = 1.0 / counts
        # Normalize weights so the first bin (typically lowest intensity) has a weight of 1.0.
        weights = (weights / weights[0]).astype(np.float16) # normalize weights

        # Assign a weight to each individual sample based on its bin.
        sample_weights = weights[bin_idx]

        self.sampler = WeightedRandomSampler(
            weights=torch.from_numpy(sample_weights),
            num_samples=n_samples,
            replacement=True
        )

    def train_dataloader(self):
        if self.sampler is None:
            return DataLoader(
                self.datasets['train'],
                batch_size=self.batch_size,
                num_workers=self.num_workers,
                shuffle=True,
                # pin_memory=True,
                persistent_workers=self.num_workers > 0,
            )
        return DataLoader(
            self.datasets['train'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            sampler=self.sampler,
            pin_memory=True,
            persistent_workers=self.num_workers > 0,
        )

    def val_dataloader(self):
        return DataLoader(
            self.datasets['val'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True,
            persistent_workers=self.num_workers > 0
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.datasets['test'],
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False,
            pin_memory=True,
            persistent_workers=self.num_workers > 0
        )

if __name__ == "__main__":
    # This block demonstrates how to instantiate and use the NowcastingDataModule.
    # It also includes a basic timing test and a utility to print the structure
    # of the data batches.

    # --- Configuration Parameters ---
    # Define the path to your Zarr dataset. Replace with your actual path.
    path = '/vol/knmimo-nobackup/users/mrobben/nowcasting/data/dataset.zarr'

    # var_info: Defines which variables to load, their transformations, and how inputs are structured.
    var_info = {
        "sample_var": "radar/max_intensity_grid", # Variable used to generate sample weights
        "in_vars": [
            # Example of nested 'in_vars': Each sub-list will be treated as a separate input group.
            # This is useful for multi-modal models that expect different data types as separate inputs.
            ["radar/rtcor"], 
            ["aws_inter/TOT_T_DRYB_10"]
            # Uncomment and adjust paths to include other satellite or harmonie data:
            # ["sat_l1p5/WV_062", "sat_l1p5/IR_108"],
            # ["harmonie/PRES_GDS0_GPML", "harmonie/DPT_GDS0_HTGL", "harmonie/R_H_GDS0_HTGL",
            #  "harmonie/A_PCP_GDS0_HTGL_acc", "harmonie/T_CDC_GDS0_HTGL", "harmonie/KNMI_var_201_entatm"],
            # If you prefer a single concatenated input tensor for all inputs, use a flat list:
            # "in_vars": ["radar/rtcor", "sat_l1p5/WV_062", "harmonie/PRES_GDS0_GPML"],
        ],
        "out_vars": [
            "radar/rtcor" # Target variable(s) for the model to predict
        ],
        # latlon: Set to True to include static latitude and longitude maps as additional input channels.
        "latlon": False, 
        "transforms": {
            # Define transformations to apply to specific Zarr groups or variables.
            # These refer to functions defined in `nowcasting.data.utils.transforms`.
            "radar": {
                "default_rainrate": {"mean": 0.030197, "std": 0.539229}, # Convert radar data to dBZ and normalize
            },
            # "harmonie": {"resize": {"scale": 2}},
            # "harmonie/TMP_GDS0_HTGL": {
            #     "normalize": {"mean": 282.99435754062006, "std": 6.236862884872817}
            # },
            "aws_inter/TOT_T_DRYB_10": {
                "normalize": {"mean": 11.296092, "std": 6.215682}
            }
        }
    }

    # split_info: Defines how the dataset is split into train/val/test sets based on time.
    split_info = {
        'split_rules': {
            "test": {"year": [2023]}, # Data from 2023 for the test set
            "val":  {"month": [6, 11]}, # Data from June and November of other years for validation
            "train": {} # All remaining data for the training set (acts as a default)
        },
        # apply_missing_masks: List of Zarr groups whose 'time_mask' should be used to filter out missing data.
        'apply_missing_masks': ['radar', 'harmonie', 'sat_l2', 'sat_l1p5', 'aws'],
        # clutter_threshold: Radar clutter score threshold. Samples with clutter score > 50 are ignored.
        'clutter_threshold': 50,
    }   

    # sample_info: Configuration for how samples are generated and optionally weighted.
    sample_info = {
        # threshold: Only include samples where the 'weight' (from 'sample_var') is above this value.
        # The value (25.0) is scaled by (255/50) in the code, so it's effectively 25.0 * (255/50) in raw units.
        'threshold': 0.01, 
        'methods': {
            # Aggregation method for generating sample weights for each split.
            # 'max_pool' means the weight is the maximum value within the sample's window.
            'train': {'agg': 'max_pool'},
            'val': {'agg': 'max_pool', 'center_crop': True}, # 'val' also applies a center crop
            'test': {'agg': 'max_pool'}
        },
        # bins: Configuration for weighted random sampling (uncomment to enable).
        # This helps in balancing datasets with rare events (e.g., heavy rain).
        # 'bins':{
        #     'n_bins': 30,       # Number of bins for sample weights
        #     'first_bin': 0.02,  # Lower bound of the first bin (scaled)
        #     'last_bin': 30,     # Upper bound of the last bin (scaled)
        #     'slope': 0.2,       # Controls the non-linear spacing of bins
        #     'scaler': 5.1,      # Scaling factor for sample weights before binning (e.g., 255/50 for radar)
        #     'n_samples': 30_000,# Number of samples to draw per epoch for training
        # },
    }

    # --- Instantiate and Setup DataModule ---
    print("Initializing NowcastingDataModule...")
    data_module = NowcastingDataModule(
        path=path,
        var_info=var_info,
        sample_info=sample_info,
        split_info=split_info,
        context_len=4,       # 4 time steps for input
        forecast_len=18,     # 18 time steps for output
        include_timestamps=True, # Include relative timestamps in context
        use_crop=[True, False],
        img_size=[(8,8), (24,22)],      # Spatial patch size: 8 blocks x 8 blocks
        stride=(1,1,1),      # Sample generation stride (t, h, w)
        batch_size=8,        # Batch size for DataLoaders
        num_workers=8,       # Number of CPU workers for data loading
    )
    print("Setting up DataModule (loading metadata, generating samples, applying transforms)...")
    data_module.setup() # This is a crucial step that prepares the datasets

    # --- Data Inspection and Timing Test ---
    print("\n--- Inspecting First Validation Batch ---")
    import time
    start = time.time()

    def print_structure(x: object, indent: int = 0):
        """
        Recursively prints the structure (shape for tensors, elements for lists/tuples)
        of a data batch. Useful for debugging and understanding complex outputs.
        """
        prefix = " " * indent
        if isinstance(x, torch.Tensor):
            print(f"{prefix}Tensor shape: {tuple(x.shape)}, dtype: {x.dtype}")
        elif isinstance(x, (list, tuple)):
            print(f"{prefix}{type(x).__name__}[")
            for elem in x:
                print_structure(elem, indent + 2)
            print(f"{prefix}]")
        else:
            print(f"{prefix}{type(x)} # not a Tensor or list/tuple")

    # Iterate through the first batch of the validation dataloader to inspect its structure and time loading.
    for i, (context, future) in enumerate(data_module.val_dataloader()):
        print(f"Batch {i}:")
        print("  Context (Input to Model):")
        print_structure(context, indent=4) # Use 4 spaces for better readability
        print("  Future (Target for Model):")
        print_structure(future, indent=4)
        if i == 0: # Only print for the first batch
            elapsed = time.time() - start
            print(f"\nTime to load first validation batch: {elapsed:.3f} seconds")
            break